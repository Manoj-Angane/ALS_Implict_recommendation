{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/manojangane/spark/spark-2.2.0-bin-hadoop2.7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "       .builder \\\n",
    "       .appName(\"RecommendationSystem\") \\\n",
    "       .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# File location and type\n",
    "file_song_data = \"/home/manojangane/song_data.csv\"\n",
    "file_triplets_data =\"/home/manojangane//Triplets.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+--------------------+--------------------+----+\n",
      "|           song_id|               title|             release|         artist_name|year|\n",
      "+------------------+--------------------+--------------------+--------------------+----+\n",
      "|SOQMMHC12AB0180CB8|        Silent Night|Monster Ballads X...|    Faster Pussy cat|2003|\n",
      "|SOVFVAK12A8C1350D9|         Tanssi vaan|         Karkuteillä|    Karkkiautomaatti|1995|\n",
      "|SOGTUKN12AB017F4F1|   No One Could Ever|              Butter|      Hudson Mohawke|2006|\n",
      "|SOBNYVR12A8C13558C|       Si Vos Querés|             De Culo|         Yerba Brava|2003|\n",
      "|SOHSBXH12A8C13B0DF|    Tangle Of Aspens|Rene Ablaze Prese...|          Der Mystic|   0|\n",
      "|SOZVAPQ12A8C13B63C|\"Symphony No. 1 G...|Berwald: Symphoni...|    David Montgomery|   0|\n",
      "|SOQVRHI12A6D4FB2D7|    We Have Got Love|Strictly The Best...|  Sasha / Turbulence|   0|\n",
      "|SOEYRFT12AB018936C|   2 Da Beat Ch'yall|             Da Bomb|          Kris Kross|1993|\n",
      "|SOPMIYT12A6D4F851E|             Goodbye|           Danny Boy|        Joseph Locke|   0|\n",
      "|SOJCFMH12A8C13B0C2|Mama_ mama can't ...|March to cadence ...|The Sun Harbor's ...|   0|\n",
      "|SOYGNWH12AB018191E|       L'antarctique|Des cobras des ta...|    3 Gars Su'l Sofa|2007|\n",
      "|SOLJTLX12AB01890ED|  El hijo del pueblo|32 Grandes Éxitos...|       Jorge Negrete|1997|\n",
      "|SOQQESG12A58A7AA28|Cold Beer feat. P...|International Har...|        Danny Diablo|   0|\n",
      "|SOMPVQB12A8C1379BB|              Pilots|           The Loyal|           Tiger Lou|2005|\n",
      "|SOGPCJI12A8C13CCA0|              N Gana|Afropea 3 - Telli...|     Waldemar Bastos|   0|\n",
      "|SOSDCFG12AB0184647|                 006|          Lena 20 År|     Lena Philipsson|1998|\n",
      "|SOBARPM12A8C133DFF|(Looking For) The...|          Cover Girl|        Shawn Colvin|1994|\n",
      "|SOKOVRQ12A8C142811|   Ethos of Coercion|Descend Into Depr...|         Dying Fetus|2009|\n",
      "|SOIMMJJ12AF72AD643|         Rock-N-Rule|I'm Only A Man (B...|               Emery|2007|\n",
      "|SOVMBTP12A8C13A8F6|       La bola extra|       La bola extra|        Los Ronaldos|   0|\n",
      "+------------------+--------------------+--------------------+--------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Uploading Song CSV file with tab delimited\n",
    "song_df = spark.read.csv(file_song_data,\n",
    "                            inferSchema ='true',\n",
    "                            header = 'true',\n",
    "                            sep=',')\n",
    "\n",
    "\n",
    "song_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+----------+\n",
      "|             user_id|            songid|play_count|\n",
      "+--------------------+------------------+----------+\n",
      "|b80344d063b5ccb32...|SOAKIMP12A8C130995|         1|\n",
      "|b80344d063b5ccb32...|SOBBMDR12A8C13253B|         2|\n",
      "|b80344d063b5ccb32...|SOBXHDL12A81C204C0|         1|\n",
      "|b80344d063b5ccb32...|SOBYHAJ12A6701BF1D|         1|\n",
      "|b80344d063b5ccb32...|SODACBL12A8C13C273|         1|\n",
      "|b80344d063b5ccb32...|SODDNQT12A6D4F5F7E|         5|\n",
      "|b80344d063b5ccb32...|SODXRTY12AB0180F3B|         1|\n",
      "|b80344d063b5ccb32...|SOFGUAY12AB017B0A8|         1|\n",
      "|b80344d063b5ccb32...|SOFRQTD12A81C233C0|         1|\n",
      "|b80344d063b5ccb32...|SOHQWYZ12A6D4FA701|         1|\n",
      "|b80344d063b5ccb32...|SOIYTOA12A6D4F9A23|         1|\n",
      "|b80344d063b5ccb32...|SOIZAZL12A6701C53B|         5|\n",
      "|b80344d063b5ccb32...|SOJNNUA12A8AE48C7A|         1|\n",
      "|b80344d063b5ccb32...|SOJPFQG12A58A7833A|         1|\n",
      "|b80344d063b5ccb32...|SOKRIMP12A6D4F5DA3|         5|\n",
      "|b80344d063b5ccb32...|SOLLGNU12AF72A4D4F|         1|\n",
      "|b80344d063b5ccb32...|SOMGIYR12AB0187973|         6|\n",
      "|b80344d063b5ccb32...|SOMLMKI12A81C204BC|         1|\n",
      "|b80344d063b5ccb32...|SOMSQJY12A8C138539|         1|\n",
      "|b80344d063b5ccb32...|SONSAEZ12A8C138D7A|         1|\n",
      "+--------------------+------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "# Creating schema\n",
    "schema = StructType([StructField('user_id', StringType()),\n",
    "                      StructField('songid', StringType()),\n",
    "                      StructField('play_count', IntegerType())])\n",
    "\n",
    "#Uploading Triplets file with tab delimited\n",
    "tri_df = spark.read.csv(file_triplets_data,\n",
    "                          schema= schema,\n",
    "                            sep='\\t')\n",
    "\n",
    "\n",
    "tri_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+----------+------------------+-----------------+-------------------+-----------------+----+\n",
      "|             user_id|            songid|play_count|           song_id|            title|            release|      artist_name|year|\n",
      "+--------------------+------------------+----------+------------------+-----------------+-------------------+-----------------+----+\n",
      "|5be795a4758aa4f13...|SOEYVHS12AB0181D31|         3|SOEYVHS12AB0181D31|          Monster|   The Fame Monster|        Lady GaGa|2009|\n",
      "|8d4f1822e21f0a91f...|SOSAUVD12A67ADE6AE|         1|SOSAUVD12A67ADE6AE| I Know It's Over|  The Queen Is Dead|       The Smiths|1986|\n",
      "|5be795a4758aa4f13...|SOHNVHC12A6D4F95AB|         6|SOHNVHC12A6D4F95AB|     Elephant Gun| The Gulag Orkestar|           Beirut|2006|\n",
      "|84cd5a870058d7d85...|SOZTCOW12A8C134269|         1|SOZTCOW12A8C134269|     Crazy On You|The Essential Heart|            Heart|   0|\n",
      "|4f90d64332d78199f...|SOXKMJJ12AC468910D|         2|SOXKMJJ12AC468910D|Hiatus (feat. NO)|Stand Up And Scream|Asking Alexandria|2009|\n",
      "+--------------------+------------------+----------+------------------+-----------------+-------------------+-----------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MSD = tri_df.join(song_df, tri_df.songid == song_df.song_id,how='left') \n",
    "MSD.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+----------+-----+------------------+-----------+----+\n",
      "|             user_id|           song_id|play_count|title|           release|artist_name|year|\n",
      "+--------------------+------------------+----------+-----+------------------+-----------+----+\n",
      "|79f93851e840f9d1f...|SOATHTW12A58A7EDB5|         1| Mutt|Enema Of The State|  Blink-182|1998|\n",
      "|043d81932e75d5749...|SOATHTW12A58A7EDB5|         5| Mutt|Enema Of The State|  Blink-182|1998|\n",
      "|ebacfcb5fa29a601f...|SOATHTW12A58A7EDB5|         1| Mutt|Enema Of The State|  Blink-182|1998|\n",
      "|417c73dd95669d191...|SOATHTW12A58A7EDB5|         1| Mutt|Enema Of The State|  Blink-182|1998|\n",
      "|52ab33fbb2fa3aeb2...|SOATHTW12A58A7EDB5|         1| Mutt|Enema Of The State|  Blink-182|1998|\n",
      "+--------------------+------------------+----------+-----+------------------+-----------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MSD = MSD['user_id','song_id','play_count','title','release','artist_name','year']\n",
    "MSD.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+----------+-----+------------------+-----------+------------+\n",
      "|             user_id|           song_id|play_count|title|           release|artist_name|release_year|\n",
      "+--------------------+------------------+----------+-----+------------------+-----------+------------+\n",
      "|79f93851e840f9d1f...|SOATHTW12A58A7EDB5|         1| Mutt|Enema Of The State|  Blink-182|        1998|\n",
      "|043d81932e75d5749...|SOATHTW12A58A7EDB5|         5| Mutt|Enema Of The State|  Blink-182|        1998|\n",
      "|ebacfcb5fa29a601f...|SOATHTW12A58A7EDB5|         1| Mutt|Enema Of The State|  Blink-182|        1998|\n",
      "|417c73dd95669d191...|SOATHTW12A58A7EDB5|         1| Mutt|Enema Of The State|  Blink-182|        1998|\n",
      "|52ab33fbb2fa3aeb2...|SOATHTW12A58A7EDB5|         1| Mutt|Enema Of The State|  Blink-182|        1998|\n",
      "+--------------------+------------------+----------+-----+------------------+-----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MSD = MSD.withColumnRenamed('year', 'release_year')\n",
    "MSD.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2086946"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of rows \n",
    "MSD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users:  3783\n"
     ]
    }
   ],
   "source": [
    "# number of distinct user_Id\n",
    "user=MSD.select(\"user_id\").distinct()\n",
    "user1,user2= user.randomSplit([0.05,0.95], seed=123)\n",
    "usercount = user1.count()\n",
    "print(\"Number of users: \", usercount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of songs:  10000\n"
     ]
    }
   ],
   "source": [
    "# number of distinct song_Id\n",
    "songs= MSD.select(\"song_id\").distinct()\n",
    "songcount=songs.count()\n",
    "print(\"Number of songs: \", songcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|             user_id|new_userid|\n",
      "+--------------------+----------+\n",
      "|126ef5859eb5e96f7...|         0|\n",
      "|1d7b9780e492c062c...|         1|\n",
      "|2c0815308dfd33b4b...|         2|\n",
      "|3d24b9ffed6a82778...|         3|\n",
      "|3dd54878cb47456b8...|         4|\n",
      "|459ab8388e7806755...|         5|\n",
      "|5e7105e485a04bf0b...|         6|\n",
      "|6bb6fa9a23505dc55...|         7|\n",
      "|739f32d4c2690554b...|         8|\n",
      "|7a3943dfa7f83e321...|         9|\n",
      "|7de4388c64742657d...|        10|\n",
      "|9619f405e777e8331...|        11|\n",
      "|a2c1d795852bd22c6...|        12|\n",
      "|a8e8fd13a2909af99...|        13|\n",
      "|aab99ec2a563f732e...|        14|\n",
      "|c06d794619168b4bf...|        15|\n",
      "|c2cfc654c54fddc9f...|        16|\n",
      "|e43304402c7407bc4...|        17|\n",
      "|e6bf98dccce485c26...|        18|\n",
      "|e7fc73d0eb0d851bc...|        19|\n",
      "+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "# Creating new columns of unique integers for user_id and song_id\n",
    "user_df = user1.withColumn(\"new_userid\", monotonically_increasing_id())\n",
    "user_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------+\n",
      "|           song_id|new_songId|\n",
      "+------------------+----------+\n",
      "|SOATHTW12A58A7EDB5|         0|\n",
      "|SOAZMXH12AB0186DDE|         1|\n",
      "|SOBAQTV12A8C142277|         2|\n",
      "|SOCKUUJ12A6D4FA41C|         3|\n",
      "|SOCUVKX12A6D4F8ED7|         4|\n",
      "|SODABFB12A58A81788|         5|\n",
      "|SODASIJ12A6D4F5D89|         6|\n",
      "|SODYTRD12A81C2329F|         7|\n",
      "|SOECOOL12AB0181A2F|         8|\n",
      "|SOECTGX12A6310E233|         9|\n",
      "|SOERLLT12AC468DAF3|        10|\n",
      "|SOGKEGN12AB0185355|        11|\n",
      "|SOGXQYC12AB0183AE5|        12|\n",
      "|SOHXDTJ12A81C219C2|        13|\n",
      "|SOICVFJ12A8AE47FF0|        14|\n",
      "|SOJGIUN12A6BD55B8E|        15|\n",
      "|SOKOVZK12A6D4F707F|        16|\n",
      "|SOKQHXV12AB0185B3D|        17|\n",
      "|SOKUAGP12A8C133B94|        18|\n",
      "|SOLIVXX12A6D4F7950|        19|\n",
      "+------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "songs_df = songs.select(\"song_id\", monotonically_increasing_id().alias('new_songId'))\n",
    "songs_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+------------------+----------+\n",
      "|             user_id|new_userid|           song_id|new_songId|\n",
      "+--------------------+----------+------------------+----------+\n",
      "|126ef5859eb5e96f7...|         0|SOATHTW12A58A7EDB5|         0|\n",
      "|126ef5859eb5e96f7...|         0|SOAZMXH12AB0186DDE|         1|\n",
      "|126ef5859eb5e96f7...|         0|SOBAQTV12A8C142277|         2|\n",
      "|126ef5859eb5e96f7...|         0|SOCKUUJ12A6D4FA41C|         3|\n",
      "|126ef5859eb5e96f7...|         0|SOCUVKX12A6D4F8ED7|         4|\n",
      "+--------------------+----------+------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Cross Join user and Songs\n",
    "crossjoin = user_df.crossJoin(songs_df)\n",
    "crossjoin.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37830000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crossjoin.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = crossjoin.join(MSD, [\"user_id\", \"song_id\"], \"left\").fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_df= df.select(df.new_userid.cast(\"int\"),df.new_songId.cast(\"int\"),df.play_count.cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the ALS hyperparameters\n",
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "model = ALS(userCol= \"new_userid\", itemCol= \"new_songId\", ratingCol= \"play_count\", rank = 10, maxIter = 10,alpha = 20, regParam = .05,  coldStartStrategy=\"drop\", nonnegative = True, implicitPrefs = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the dataframe into training and test data\n",
    "(train_data, test_data) = model_df.select('new_userid','new_songId','play_count').randomSplit([0.7, 0.3], seed=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " #Expected percentile rank error metric function\n",
    "def ROEM(predictions, userCol = \"new_userid\", itemCol = \"new_songId\", ratingCol = \"play_count\"):\n",
    "  #Creates table that can be queried\n",
    "  predictions.createOrReplaceTempView(\"predictions\")\n",
    "\n",
    "  #Sum of total number of plays of all songs\n",
    "  denominator = predictions.groupBy().sum(ratingCol).collect()[0][0]\n",
    "\n",
    "  #Calculating rankings of songs predictions by user\n",
    "  spark.sql(\"SELECT \" + userCol + \" , \" + ratingCol + \" , PERCENT_RANK() OVER (PARTITION BY \" + userCol + \" ORDER BY prediction DESC) AS rank FROM predictions\").createOrReplaceTempView(\"rankings\")\n",
    "\n",
    "   #Multiplies the rank of each song by the number of plays and adds the products together\n",
    "  numerator = spark.sql('SELECT SUM(' + ratingCol + ' * rank) FROM rankings').collect()[0][0]\n",
    "\n",
    "  performance = numerator/denominator\n",
    "\n",
    "  return performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[new_userid: int, new_songId: int, play_count: int]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o101.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 76.0 failed 1 times, most recent failure: Lost task 0.0 in stage 76.0 (TID 83485, localhost, executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat scala.collection.mutable.ArrayBuilder$ofInt.mkArray(ArrayBuilder.scala:323)\n\tat scala.collection.mutable.ArrayBuilder$ofInt.resize(ArrayBuilder.scala:329)\n\tat scala.collection.mutable.ArrayBuilder$ofInt.ensureSize(ArrayBuilder.scala:341)\n\tat scala.collection.mutable.ArrayBuilder$ofInt.$plus$eq(ArrayBuilder.scala:346)\n\tat scala.collection.mutable.ArrayBuilder$ofInt.$plus$eq(ArrayBuilder.scala:316)\n\tat scala.collection.generic.Growable$$anonfun$$plus$plus$eq$1.apply(Growable.scala:59)\n\tat scala.collection.generic.Growable$$anonfun$$plus$plus$eq$1.apply(Growable.scala:59)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofInt.foreach(ArrayOps.scala:234)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuilder$ofInt.$plus$plus$eq(ArrayBuilder.scala:359)\n\tat scala.collection.mutable.ArrayBuilder$ofInt.$plus$plus$eq(ArrayBuilder.scala:316)\n\tat org.apache.spark.ml.recommendation.ALS$UncompressedInBlockBuilder.add(ALS.scala:1158)\n\tat org.apache.spark.ml.recommendation.ALS$$anonfun$23$$anonfun$apply$16.apply(ALS.scala:1376)\n\tat org.apache.spark.ml.recommendation.ALS$$anonfun$23$$anonfun$apply$16.apply(ALS.scala:1375)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.util.collection.CompactBuffer$$anon$1.foreach(CompactBuffer.scala:115)\n\tat scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\n\tat org.apache.spark.util.collection.CompactBuffer.foreach(CompactBuffer.scala:30)\n\tat org.apache.spark.ml.recommendation.ALS$$anonfun$23.apply(ALS.scala:1375)\n\tat org.apache.spark.ml.recommendation.ALS$$anonfun$23.apply(ALS.scala:1372)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$mapValues$1$$anonfun$apply$40$$anonfun$apply$41.apply(PairRDDFunctions.scala:760)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$mapValues$1$$anonfun$apply$40$$anonfun$apply$41.apply(PairRDDFunctions.scala:760)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1038)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1029)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:969)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1029)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:760)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2087)\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1158)\n\tat org.apache.spark.ml.recommendation.ALS$.train(ALS.scala:857)\n\tat org.apache.spark.ml.recommendation.ALS.fit(ALS.scala:622)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat scala.collection.mutable.ArrayBuilder$ofInt.mkArray(ArrayBuilder.scala:323)\n\tat scala.collection.mutable.ArrayBuilder$ofInt.resize(ArrayBuilder.scala:329)\n\tat scala.collection.mutable.ArrayBuilder$ofInt.ensureSize(ArrayBuilder.scala:341)\n\tat scala.collection.mutable.ArrayBuilder$ofInt.$plus$eq(ArrayBuilder.scala:346)\n\tat scala.collection.mutable.ArrayBuilder$ofInt.$plus$eq(ArrayBuilder.scala:316)\n\tat scala.collection.generic.Growable$$anonfun$$plus$plus$eq$1.apply(Growable.scala:59)\n\tat scala.collection.generic.Growable$$anonfun$$plus$plus$eq$1.apply(Growable.scala:59)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofInt.foreach(ArrayOps.scala:234)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuilder$ofInt.$plus$plus$eq(ArrayBuilder.scala:359)\n\tat scala.collection.mutable.ArrayBuilder$ofInt.$plus$plus$eq(ArrayBuilder.scala:316)\n\tat org.apache.spark.ml.recommendation.ALS$UncompressedInBlockBuilder.add(ALS.scala:1158)\n\tat org.apache.spark.ml.recommendation.ALS$$anonfun$23$$anonfun$apply$16.apply(ALS.scala:1376)\n\tat org.apache.spark.ml.recommendation.ALS$$anonfun$23$$anonfun$apply$16.apply(ALS.scala:1375)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.util.collection.CompactBuffer$$anon$1.foreach(CompactBuffer.scala:115)\n\tat scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\n\tat org.apache.spark.util.collection.CompactBuffer.foreach(CompactBuffer.scala:30)\n\tat org.apache.spark.ml.recommendation.ALS$$anonfun$23.apply(ALS.scala:1375)\n\tat org.apache.spark.ml.recommendation.ALS$$anonfun$23.apply(ALS.scala:1372)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$mapValues$1$$anonfun$apply$40$$anonfun$apply$41.apply(PairRDDFunctions.scala:760)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$mapValues$1$$anonfun$apply$40$$anonfun$apply$41.apply(PairRDDFunctions.scala:760)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1038)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1029)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:969)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1029)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:760)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-2013cb48fc1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Fits model to fold within training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfitted_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/spark/spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m~/spark/spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \"\"\"\n\u001b[1;32m    261\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o101.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 76.0 failed 1 times, most recent failure: Lost task 0.0 in stage 76.0 (TID 83485, localhost, executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat scala.collection.mutable.ArrayBuilder$ofInt.mkArray(ArrayBuilder.scala:323)\n\tat scala.collection.mutable.ArrayBuilder$ofInt.resize(ArrayBuilder.scala:329)\n\tat scala.collection.mutable.ArrayBuilder$ofInt.ensureSize(ArrayBuilder.scala:341)\n\tat scala.collection.mutable.ArrayBuilder$ofInt.$plus$eq(ArrayBuilder.scala:346)\n\tat scala.collection.mutable.ArrayBuilder$ofInt.$plus$eq(ArrayBuilder.scala:316)\n\tat scala.collection.generic.Growable$$anonfun$$plus$plus$eq$1.apply(Growable.scala:59)\n\tat scala.collection.generic.Growable$$anonfun$$plus$plus$eq$1.apply(Growable.scala:59)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofInt.foreach(ArrayOps.scala:234)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuilder$ofInt.$plus$plus$eq(ArrayBuilder.scala:359)\n\tat scala.collection.mutable.ArrayBuilder$ofInt.$plus$plus$eq(ArrayBuilder.scala:316)\n\tat org.apache.spark.ml.recommendation.ALS$UncompressedInBlockBuilder.add(ALS.scala:1158)\n\tat org.apache.spark.ml.recommendation.ALS$$anonfun$23$$anonfun$apply$16.apply(ALS.scala:1376)\n\tat org.apache.spark.ml.recommendation.ALS$$anonfun$23$$anonfun$apply$16.apply(ALS.scala:1375)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.util.collection.CompactBuffer$$anon$1.foreach(CompactBuffer.scala:115)\n\tat scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\n\tat org.apache.spark.util.collection.CompactBuffer.foreach(CompactBuffer.scala:30)\n\tat org.apache.spark.ml.recommendation.ALS$$anonfun$23.apply(ALS.scala:1375)\n\tat org.apache.spark.ml.recommendation.ALS$$anonfun$23.apply(ALS.scala:1372)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$mapValues$1$$anonfun$apply$40$$anonfun$apply$41.apply(PairRDDFunctions.scala:760)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$mapValues$1$$anonfun$apply$40$$anonfun$apply$41.apply(PairRDDFunctions.scala:760)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1038)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1029)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:969)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1029)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:760)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2087)\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1158)\n\tat org.apache.spark.ml.recommendation.ALS$.train(ALS.scala:857)\n\tat org.apache.spark.ml.recommendation.ALS.fit(ALS.scala:622)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat scala.collection.mutable.ArrayBuilder$ofInt.mkArray(ArrayBuilder.scala:323)\n\tat scala.collection.mutable.ArrayBuilder$ofInt.resize(ArrayBuilder.scala:329)\n\tat scala.collection.mutable.ArrayBuilder$ofInt.ensureSize(ArrayBuilder.scala:341)\n\tat scala.collection.mutable.ArrayBuilder$ofInt.$plus$eq(ArrayBuilder.scala:346)\n\tat scala.collection.mutable.ArrayBuilder$ofInt.$plus$eq(ArrayBuilder.scala:316)\n\tat scala.collection.generic.Growable$$anonfun$$plus$plus$eq$1.apply(Growable.scala:59)\n\tat scala.collection.generic.Growable$$anonfun$$plus$plus$eq$1.apply(Growable.scala:59)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofInt.foreach(ArrayOps.scala:234)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuilder$ofInt.$plus$plus$eq(ArrayBuilder.scala:359)\n\tat scala.collection.mutable.ArrayBuilder$ofInt.$plus$plus$eq(ArrayBuilder.scala:316)\n\tat org.apache.spark.ml.recommendation.ALS$UncompressedInBlockBuilder.add(ALS.scala:1158)\n\tat org.apache.spark.ml.recommendation.ALS$$anonfun$23$$anonfun$apply$16.apply(ALS.scala:1376)\n\tat org.apache.spark.ml.recommendation.ALS$$anonfun$23$$anonfun$apply$16.apply(ALS.scala:1375)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.util.collection.CompactBuffer$$anon$1.foreach(CompactBuffer.scala:115)\n\tat scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\n\tat org.apache.spark.util.collection.CompactBuffer.foreach(CompactBuffer.scala:30)\n\tat org.apache.spark.ml.recommendation.ALS$$anonfun$23.apply(ALS.scala:1375)\n\tat org.apache.spark.ml.recommendation.ALS$$anonfun$23.apply(ALS.scala:1372)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$mapValues$1$$anonfun$apply$40$$anonfun$apply$41.apply(PairRDDFunctions.scala:760)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$mapValues$1$$anonfun$apply$40$$anonfun$apply$41.apply(PairRDDFunctions.scala:760)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1038)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1029)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:969)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1029)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:760)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n"
     ]
    }
   ],
   "source": [
    "# Fits model to fold within training data\n",
    "fitted_model = model.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generates predictions using fitted_model on respective CV test data\n",
    "predictions = fitted_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generates and prints a ROEM metric CV test data\n",
    "validation_performance = ROEM(predictions)\n",
    "print(validation_performance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
